{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.639, 0.099, 0.   ],\n",
       "       [0.025, 0.686, 0.   ],\n",
       "       [0.275, 0.544, 0.   ],\n",
       "       ...,\n",
       "       [0.55 , 0.062, 0.   ],\n",
       "       [0.455, 0.138, 0.   ],\n",
       "       [0.315, 0.207, 0.   ]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Loading traindata from csv, read and display storing in trainData\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import os\n",
    "trainData = np.genfromtxt(\"/Users/panigrahirenuka/PythonPractice/JupyterNotebook/trainData.csv\", delimiter=\",\", skip_header=1)\n",
    "trainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column labels.\n",
    "# These are used only to print the tree.\n",
    "header = [\"Attribute1\", \"Attribute1\", \"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_vals(rows, col):\n",
    "    \"\"\"Find the unique values for a column in a dataset.\"\"\"\n",
    "    return set([row[col] for row in rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0,\n",
       " 0.001,\n",
       " 0.002,\n",
       " 0.003,\n",
       " 0.004,\n",
       " 0.006,\n",
       " 0.008,\n",
       " 0.01,\n",
       " 0.011,\n",
       " 0.014,\n",
       " 0.019,\n",
       " 0.02,\n",
       " 0.024,\n",
       " 0.025,\n",
       " 0.026,\n",
       " 0.027,\n",
       " 0.029,\n",
       " 0.03,\n",
       " 0.032,\n",
       " 0.04,\n",
       " 0.046,\n",
       " 0.047,\n",
       " 0.05,\n",
       " 0.051,\n",
       " 0.054,\n",
       " 0.057,\n",
       " 0.058,\n",
       " 0.059,\n",
       " 0.064,\n",
       " 0.069,\n",
       " 0.07,\n",
       " 0.071,\n",
       " 0.073,\n",
       " 0.074,\n",
       " 0.075,\n",
       " 0.076,\n",
       " 0.077,\n",
       " 0.08,\n",
       " 0.082,\n",
       " 0.085,\n",
       " 0.086,\n",
       " 0.087,\n",
       " 0.091,\n",
       " 0.092,\n",
       " 0.093,\n",
       " 0.097,\n",
       " 0.098,\n",
       " 0.101,\n",
       " 0.102,\n",
       " 0.103,\n",
       " 0.106,\n",
       " 0.107,\n",
       " 0.108,\n",
       " 0.11,\n",
       " 0.112,\n",
       " 0.115,\n",
       " 0.118,\n",
       " 0.12,\n",
       " 0.121,\n",
       " 0.124,\n",
       " 0.126,\n",
       " 0.127,\n",
       " 0.128,\n",
       " 0.132,\n",
       " 0.134,\n",
       " 0.136,\n",
       " 0.138,\n",
       " 0.148,\n",
       " 0.149,\n",
       " 0.152,\n",
       " 0.153,\n",
       " 0.155,\n",
       " 0.157,\n",
       " 0.158,\n",
       " 0.163,\n",
       " 0.165,\n",
       " 0.166,\n",
       " 0.168,\n",
       " 0.17,\n",
       " 0.171,\n",
       " 0.174,\n",
       " 0.176,\n",
       " 0.179,\n",
       " 0.181,\n",
       " 0.182,\n",
       " 0.184,\n",
       " 0.185,\n",
       " 0.186,\n",
       " 0.19,\n",
       " 0.193,\n",
       " 0.194,\n",
       " 0.199,\n",
       " 0.201,\n",
       " 0.204,\n",
       " 0.207,\n",
       " 0.208,\n",
       " 0.21,\n",
       " 0.211,\n",
       " 0.212,\n",
       " 0.213,\n",
       " 0.214,\n",
       " 0.216,\n",
       " 0.218,\n",
       " 0.219,\n",
       " 0.22,\n",
       " 0.221,\n",
       " 0.222,\n",
       " 0.223,\n",
       " 0.225,\n",
       " 0.226,\n",
       " 0.227,\n",
       " 0.228,\n",
       " 0.229,\n",
       " 0.23,\n",
       " 0.231,\n",
       " 0.233,\n",
       " 0.235,\n",
       " 0.238,\n",
       " 0.239,\n",
       " 0.244,\n",
       " 0.245,\n",
       " 0.246,\n",
       " 0.247,\n",
       " 0.249,\n",
       " 0.25,\n",
       " 0.251,\n",
       " 0.252,\n",
       " 0.255,\n",
       " 0.259,\n",
       " 0.262,\n",
       " 0.263,\n",
       " 0.264,\n",
       " 0.265,\n",
       " 0.267,\n",
       " 0.268,\n",
       " 0.27,\n",
       " 0.271,\n",
       " 0.272,\n",
       " 0.274,\n",
       " 0.275,\n",
       " 0.276,\n",
       " 0.277,\n",
       " 0.278,\n",
       " 0.279,\n",
       " 0.283,\n",
       " 0.286,\n",
       " 0.287,\n",
       " 0.288,\n",
       " 0.289,\n",
       " 0.291,\n",
       " 0.296,\n",
       " 0.297,\n",
       " 0.298,\n",
       " 0.299,\n",
       " 0.3,\n",
       " 0.301,\n",
       " 0.304,\n",
       " 0.305,\n",
       " 0.306,\n",
       " 0.308,\n",
       " 0.309,\n",
       " 0.312,\n",
       " 0.314,\n",
       " 0.315,\n",
       " 0.316,\n",
       " 0.32,\n",
       " 0.322,\n",
       " 0.324,\n",
       " 0.327,\n",
       " 0.328,\n",
       " 0.33,\n",
       " 0.331,\n",
       " 0.332,\n",
       " 0.333,\n",
       " 0.335,\n",
       " 0.336,\n",
       " 0.337,\n",
       " 0.338,\n",
       " 0.339,\n",
       " 0.34,\n",
       " 0.341,\n",
       " 0.347,\n",
       " 0.348,\n",
       " 0.35,\n",
       " 0.352,\n",
       " 0.364,\n",
       " 0.365,\n",
       " 0.37,\n",
       " 0.371,\n",
       " 0.379,\n",
       " 0.38,\n",
       " 0.382,\n",
       " 0.385,\n",
       " 0.394,\n",
       " 0.396,\n",
       " 0.397,\n",
       " 0.399,\n",
       " 0.4,\n",
       " 0.401,\n",
       " 0.402,\n",
       " 0.403,\n",
       " 0.404,\n",
       " 0.406,\n",
       " 0.408,\n",
       " 0.414,\n",
       " 0.416,\n",
       " 0.419,\n",
       " 0.42,\n",
       " 0.422,\n",
       " 0.423,\n",
       " 0.424,\n",
       " 0.425,\n",
       " 0.426,\n",
       " 0.428,\n",
       " 0.43,\n",
       " 0.431,\n",
       " 0.432,\n",
       " 0.435,\n",
       " 0.436,\n",
       " 0.438,\n",
       " 0.44,\n",
       " 0.442,\n",
       " 0.443,\n",
       " 0.449,\n",
       " 0.451,\n",
       " 0.454,\n",
       " 0.455,\n",
       " 0.456,\n",
       " 0.458,\n",
       " 0.459,\n",
       " 0.46,\n",
       " 0.462,\n",
       " 0.465,\n",
       " 0.467,\n",
       " 0.469,\n",
       " 0.471,\n",
       " 0.473,\n",
       " 0.475,\n",
       " 0.478,\n",
       " 0.483,\n",
       " 0.484,\n",
       " 0.486,\n",
       " 0.489,\n",
       " 0.491,\n",
       " 0.492,\n",
       " 0.494,\n",
       " 0.5,\n",
       " 0.501,\n",
       " 0.503,\n",
       " 0.505,\n",
       " 0.506,\n",
       " 0.51,\n",
       " 0.511,\n",
       " 0.515,\n",
       " 0.518,\n",
       " 0.521,\n",
       " 0.523,\n",
       " 0.525,\n",
       " 0.529,\n",
       " 0.53,\n",
       " 0.531,\n",
       " 0.534,\n",
       " 0.536,\n",
       " 0.537,\n",
       " 0.538,\n",
       " 0.539,\n",
       " 0.54,\n",
       " 0.542,\n",
       " 0.543,\n",
       " 0.545,\n",
       " 0.546,\n",
       " 0.548,\n",
       " 0.55,\n",
       " 0.552,\n",
       " 0.553,\n",
       " 0.556,\n",
       " 0.557,\n",
       " 0.561,\n",
       " 0.563,\n",
       " 0.564,\n",
       " 0.569,\n",
       " 0.571,\n",
       " 0.574,\n",
       " 0.575,\n",
       " 0.577,\n",
       " 0.579,\n",
       " 0.58,\n",
       " 0.582,\n",
       " 0.583,\n",
       " 0.584,\n",
       " 0.585,\n",
       " 0.588,\n",
       " 0.589,\n",
       " 0.594,\n",
       " 0.595,\n",
       " 0.6,\n",
       " 0.603,\n",
       " 0.604,\n",
       " 0.605,\n",
       " 0.606,\n",
       " 0.607,\n",
       " 0.609,\n",
       " 0.61,\n",
       " 0.612,\n",
       " 0.613,\n",
       " 0.616,\n",
       " 0.619,\n",
       " 0.621,\n",
       " 0.622,\n",
       " 0.625,\n",
       " 0.627,\n",
       " 0.629,\n",
       " 0.63,\n",
       " 0.631,\n",
       " 0.633,\n",
       " 0.634,\n",
       " 0.635,\n",
       " 0.636,\n",
       " 0.637,\n",
       " 0.638,\n",
       " 0.639,\n",
       " 0.64,\n",
       " 0.641,\n",
       " 0.643,\n",
       " 0.646,\n",
       " 0.648,\n",
       " 0.65,\n",
       " 0.652,\n",
       " 0.654,\n",
       " 0.655,\n",
       " 0.656,\n",
       " 0.66,\n",
       " 0.661,\n",
       " 0.663,\n",
       " 0.664,\n",
       " 0.668,\n",
       " 0.669,\n",
       " 0.67,\n",
       " 0.671,\n",
       " 0.672,\n",
       " 0.673,\n",
       " 0.675,\n",
       " 0.677,\n",
       " 0.679,\n",
       " 0.682,\n",
       " 0.685,\n",
       " 0.687,\n",
       " 0.689,\n",
       " 0.691,\n",
       " 0.692,\n",
       " 0.694,\n",
       " 0.696,\n",
       " 0.697,\n",
       " 0.698,\n",
       " 0.699,\n",
       " 0.7,\n",
       " 0.701,\n",
       " 0.703,\n",
       " 0.704,\n",
       " 0.705,\n",
       " 0.706,\n",
       " 0.707,\n",
       " 0.709,\n",
       " 0.715,\n",
       " 0.716,\n",
       " 0.718,\n",
       " 0.719,\n",
       " 0.72,\n",
       " 0.721,\n",
       " 0.725,\n",
       " 0.729,\n",
       " 0.73,\n",
       " 0.731,\n",
       " 0.734,\n",
       " 0.736,\n",
       " 0.737,\n",
       " 0.74,\n",
       " 0.741,\n",
       " 0.742,\n",
       " 0.744,\n",
       " 0.746,\n",
       " 0.748,\n",
       " 0.749,\n",
       " 0.751,\n",
       " 0.752,\n",
       " 0.753,\n",
       " 0.754,\n",
       " 0.756,\n",
       " 0.758,\n",
       " 0.761,\n",
       " 0.762,\n",
       " 0.763,\n",
       " 0.764,\n",
       " 0.766,\n",
       " 0.767,\n",
       " 0.77,\n",
       " 0.771,\n",
       " 0.772,\n",
       " 0.776,\n",
       " 0.779,\n",
       " 0.78,\n",
       " 0.781,\n",
       " 0.783,\n",
       " 0.784,\n",
       " 0.785,\n",
       " 0.787,\n",
       " 0.788,\n",
       " 0.791,\n",
       " 0.792,\n",
       " 0.793,\n",
       " 0.795,\n",
       " 0.796,\n",
       " 0.798,\n",
       " 0.801,\n",
       " 0.802,\n",
       " 0.806,\n",
       " 0.807,\n",
       " 0.808,\n",
       " 0.809,\n",
       " 0.811,\n",
       " 0.814,\n",
       " 0.817,\n",
       " 0.818,\n",
       " 0.819,\n",
       " 0.822,\n",
       " 0.827,\n",
       " 0.829,\n",
       " 0.831,\n",
       " 0.832,\n",
       " 0.834,\n",
       " 0.835,\n",
       " 0.836,\n",
       " 0.843,\n",
       " 0.846,\n",
       " 0.847,\n",
       " 0.848,\n",
       " 0.849,\n",
       " 0.85,\n",
       " 0.851,\n",
       " 0.852,\n",
       " 0.853,\n",
       " 0.857,\n",
       " 0.858,\n",
       " 0.859,\n",
       " 0.86,\n",
       " 0.861,\n",
       " 0.862,\n",
       " 0.864,\n",
       " 0.865,\n",
       " 0.868,\n",
       " 0.871,\n",
       " 0.872,\n",
       " 0.873,\n",
       " 0.876,\n",
       " 0.878,\n",
       " 0.879,\n",
       " 0.88,\n",
       " 0.881,\n",
       " 0.882,\n",
       " 0.883,\n",
       " 0.884,\n",
       " 0.89,\n",
       " 0.891,\n",
       " 0.892,\n",
       " 0.897,\n",
       " 0.898,\n",
       " 0.899,\n",
       " 0.901,\n",
       " 0.903,\n",
       " 0.904,\n",
       " 0.905,\n",
       " 0.908,\n",
       " 0.913,\n",
       " 0.915,\n",
       " 0.916,\n",
       " 0.923,\n",
       " 0.924,\n",
       " 0.926,\n",
       " 0.929,\n",
       " 0.93,\n",
       " 0.931,\n",
       " 0.933,\n",
       " 0.934,\n",
       " 0.935,\n",
       " 0.936,\n",
       " 0.937,\n",
       " 0.939,\n",
       " 0.94,\n",
       " 0.941,\n",
       " 0.943,\n",
       " 0.945,\n",
       " 0.947,\n",
       " 0.948,\n",
       " 0.95,\n",
       " 0.951,\n",
       " 0.953,\n",
       " 0.954,\n",
       " 0.957,\n",
       " 0.958,\n",
       " 0.961,\n",
       " 0.962,\n",
       " 0.963,\n",
       " 0.965,\n",
       " 0.967,\n",
       " 0.969,\n",
       " 0.97,\n",
       " 0.971,\n",
       " 0.973,\n",
       " 0.974,\n",
       " 0.976,\n",
       " 0.977,\n",
       " 0.981,\n",
       " 0.984,\n",
       " 0.985,\n",
       " 0.987,\n",
       " 0.988,\n",
       " 0.99,\n",
       " 0.994,\n",
       " 0.995,\n",
       " 0.996,\n",
       " 0.998,\n",
       " 0.999,\n",
       " 1.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######\n",
    "# Demo:\n",
    "unique_vals(trainData, 0)\n",
    "# unique_vals(trainData, 1)\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_counts(rows):\n",
    "    \"\"\"Counts the number of each type of example in a dataset.\"\"\"\n",
    "    counts = {}  # a dictionary of label -> count.\n",
    "    for row in rows:\n",
    "        # in our dataset format, the label is always the last column\n",
    "        label = row[-1]\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        counts[label] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 269, 1.0: 481}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######\n",
    "# Demo:\n",
    "class_counts(trainData)\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(value):\n",
    "    \"\"\"Test if a value is numeric.\"\"\"\n",
    "    return isinstance(value, int) or isinstance(value, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######\n",
    "# Demo:\n",
    "is_numeric(7)\n",
    "# is_numeric(\"Red\")\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question:\n",
    "    \"\"\"A Question is used to partition a dataset.\n",
    "\n",
    "    This class just records a 'column number' (e.g., 0 for Color) and a\n",
    "    'column value' (e.g., Green). The 'match' method is used to compare\n",
    "    the feature value in an example to the feature value stored in the\n",
    "    question. See the demo below.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, column, value):\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "\n",
    "    def match(self, example):\n",
    "        # Compare the feature value in an example to the\n",
    "        # feature value in this question.\n",
    "        val = example[self.column]\n",
    "        if is_numeric(val):\n",
    "            return val >= self.value\n",
    "        else:\n",
    "            return val == self.value\n",
    "\n",
    "    def __repr__(self):\n",
    "        # This is just a helper method to print\n",
    "        # the question in a readable format.\n",
    "        condition = \"==\"\n",
    "        if is_numeric(self.value):\n",
    "            condition = \">=\"\n",
    "        return \"Is %s %s %s?\" % (\n",
    "            header[self.column], condition, str(self.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
